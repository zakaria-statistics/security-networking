# Kubernetes iptables Lab
> Trace kube-proxy's KUBE-* chains, watch rules change with pod scaling, and debug Service routing failures

## Table of Contents
1. [Lab Environment](#1-lab-environment) - Node access and tools
2. [Phase 1: Baseline — How Many Rules Does kube-proxy Manage?](#2-phase-1-baseline--how-many-rules-does-kube-proxy-manage) - Rule counts and chain structure
3. [Phase 2: ClusterIP — Trace a Virtual IP to a Pod](#3-phase-2-clusterip--trace-a-virtual-ip-to-a-pod) - KUBE-SERVICES → KUBE-SVC → KUBE-SEP
4. [Phase 3: Probability Load Balancing — Watch Math in Action](#4-phase-3-probability-load-balancing--watch-math-in-action) - How traffic is distributed across pods
5. [Phase 4: NodePort — External Traffic to Pod](#5-phase-4-nodeport--external-traffic-to-pod) - KUBE-NODEPORTS and cross-node MASQUERADE
6. [Phase 5: Scale and Watch — Rules Change Live](#6-phase-5-scale-and-watch--rules-change-live) - kube-proxy updates chains in real-time
7. [Phase 6: Conntrack in Kubernetes](#7-phase-6-conntrack-in-kubernetes) - NAT mapping inspection at cluster scale
8. [Phase 7: NetworkPolicy — CNI Enforces, Not kube-proxy](#8-phase-7-networkpolicy--cni-enforces-not-kube-proxy) - Where cali-* chains appear
9. [Phase 8: Debug "Service Not Reachable"](#9-phase-8-debug-service-not-reachable) - Systematic failure isolation
10. [Phase 9: IPVS — When iptables Is Replaced](#10-phase-9-ipvs--when-iptables-is-replaced) - Check your cluster's mode
11. [Cleanup](#11-cleanup)

---

## 1. Lab Environment

```
┌─────────────────────────────────────────────────────────────────┐
│  K8s Cluster (k3s, kubeadm, or similar)                        │
│                                                                  │
│  Node Network:   192.168.11.0/24  (your LAN)                   │
│  Pod Network:    10.42.0.0/16     (Flannel) or 10.244.0.0/16   │
│  Service Network: 10.96.0.0/12   (default)                      │
│                                                                  │
│  You need:                                                       │
│  - SSH or kubectl exec access to a worker node                  │
│  - kubectl access with cluster-admin privileges                 │
└─────────────────────────────────────────────────────────────────┘
```

**Tools used:**
- `iptables-save` — dump all rules at once (faster than `iptables -L`)
- `iptables -t nat -L <chain> -n -v` — inspect a specific chain
- `conntrack -L` — list NAT connection table
- `kubectl get svc / endpoints / pods` — correlate rules with cluster objects
- `watch -n 2 '<cmd>'` — run command every 2 seconds

**Accessing a node:**

```bash
# Option A: SSH into node directly
ssh user@<node-ip>

# Option B: kubectl debug (works on most clusters)
kubectl debug node/<node-name> -it --image=nicolaka/netshoot
# Then in the container:
chroot /host
```

**Convention:**
- `CTL#` = run on your workstation with kubectl
- `N#` = run on the K8s node (root/sudo)

> **First:** Confirm you're on a node with iptables access:
> ```bash
> N# iptables-save | wc -l
> ```
> If you get a number greater than 50, you're in the right place.

---

## 2. Phase 1: Baseline — How Many Rules Does kube-proxy Manage?

### Step 1 — Count total rules

```bash
N# iptables-save | wc -l
```

Small cluster (few services): **100–300 lines**
Medium cluster: **1,000–5,000 lines**
Large cluster: **10,000+ lines**

### Step 2 — Count KUBE-* chains

```bash
N# iptables-save | grep "^:" | grep KUBE
```

**Each line is one chain.** Expected chains:
```
:KUBE-FIREWALL
:KUBE-FORWARD
:KUBE-KUBELET-SEPARATE-CHAINS
:KUBE-NODEPORTS
:KUBE-POSTROUTING
:KUBE-PROXY-FIREWALL
:KUBE-SEP-XXXX     ← one per pod endpoint (many of these)
:KUBE-SERVICES
:KUBE-SVC-XXXX     ← one per Service (many of these)
```

### Step 3 — Count Services programmed in iptables

```bash
N# iptables-save -t nat | grep "^-A KUBE-SERVICES" | wc -l
```

Compare to kubectl:

```bash
CTL# kubectl get svc --all-namespaces | wc -l
```

The numbers should be close (every Service gets at least one KUBE-SERVICES entry).

### Step 4 — Find where kube-proxy hooks in

```bash
N# iptables -t nat -L PREROUTING -n -v --line-numbers
```

**You should see:**
```
1  KUBE-SERVICES  all  (matches all traffic entering PREROUTING)
```

And in OUTPUT:

```bash
N# iptables -t nat -L OUTPUT -n -v --line-numbers
```

```
1  KUBE-SERVICES  all  (same chain — covers locally-generated traffic)
```

**Why both PREROUTING and OUTPUT?**
- `PREROUTING`: catches packets arriving from outside the pod
- `OUTPUT`: catches packets generated by processes on this node (e.g., a pod on this node talking to a Service)

---

## 3. Phase 2: ClusterIP — Trace a Virtual IP to a Pod

### Step 1 — Deploy a test workload

```bash
CTL# kubectl create deployment web --image=nginx --replicas=2
CTL# kubectl expose deployment web --port=80 --type=ClusterIP
CTL# kubectl get svc web
# NAME   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
# web    ClusterIP   10.96.45.123    <none>        80/TCP    10s
```

### Step 2 — Find the KUBE-SERVICES rule for this Service

```bash
N# SVC_IP=$(kubectl get svc web -o jsonpath='{.spec.clusterIP}')
N# iptables-save -t nat | grep "$SVC_IP"
```

**You should see:**
```
-A KUBE-SERVICES -d 10.96.45.123/32 -p tcp -m tcp --dport 80 -j KUBE-SVC-XXXXXX
```

Note the chain name: `KUBE-SVC-XXXXXX` — this is the per-service load-balancing chain.

### Step 3 — Follow the load-balancing chain

```bash
N# SVC_CHAIN=$(iptables-save -t nat | grep "$SVC_IP" | awk '{print $NF}')
N# iptables-save -t nat | grep "$SVC_CHAIN"
```

**With 2 replicas, you'll see something like:**
```
-A KUBE-SVC-XXXXXX -m statistic --mode random --probability 0.50000 -j KUBE-SEP-AAAA
-A KUBE-SVC-XXXXXX -j KUBE-SEP-BBBB
```

**Read this as:**
- 50% of packets → KUBE-SEP-AAAA (pod 1)
- Remaining 50% → KUBE-SEP-BBBB (pod 2)

### Step 4 — Follow an endpoint chain to the DNAT rule

```bash
N# iptables-save -t nat | grep "KUBE-SEP-AAAA"
```

**You'll see:**
```
-A KUBE-SEP-AAAA -p tcp -j DNAT --to-destination 10.42.1.5:80
```

This is the actual DNAT rule. `10.42.1.5:80` is the pod's real IP and port.

### Step 5 — Verify the pod IPs match

```bash
CTL# kubectl get endpoints web
# NAME   ENDPOINTS                   AGE
# web    10.42.1.5:80,10.42.2.3:80   1m
```

The IPs in the KUBE-SEP chains should **exactly match** the Endpoints resource. If they don't, kube-proxy hasn't synced yet.

### Step 6 — Try to ping the ClusterIP (it won't work)

```bash
N# ping -c 3 10.96.45.123
```

**Expected:** `Destination Host Unreachable` or 100% loss.

**Why?** No interface owns this IP. The DNAT rule in KUBE-SERVICES only fires for TCP port 80. ICMP packets fall through with no match → the kernel has no route to an IP that doesn't exist on any interface.

### Step 7 — But TCP works

```bash
N# curl -s http://10.96.45.123
```

**Works because:** TCP to port 80 matches the KUBE-SERVICES rule → DNAT fires → packet goes to a real pod IP that does exist.

---

## 4. Phase 3: Probability Load Balancing — Watch Math in Action

### Step 1 — Scale to 3 replicas

```bash
CTL# kubectl scale deployment web --replicas=3
```

Wait ~5 seconds for kube-proxy to update rules.

### Step 2 — Check the updated probability rules

```bash
N# iptables-save -t nat | grep "$SVC_CHAIN"
```

**With 3 pods:**
```
-A KUBE-SVC-XXXXXX -m statistic --mode random --probability 0.33333 -j KUBE-SEP-AAAA
-A KUBE-SVC-XXXXXX -m statistic --mode random --probability 0.50000 -j KUBE-SEP-BBBB
-A KUBE-SVC-XXXXXX                                                   -j KUBE-SEP-CCCC
```

**Why does 3 pods use 0.33 / 0.50 / (implicit 1.0)?**

Rules are evaluated sequentially:
- Rule 1: 33.3% chance → Pod 1 → 33.3% of all traffic goes here
- Rule 2: 50% of remaining 66.7% → Pod 2 → 33.3% of all traffic
- Rule 3: 100% of remaining 33.3% → Pod 3 → 33.3% of all traffic

Result: each pod gets exactly 1/3.

### Step 3 — Scale to 4 and re-check probabilities

```bash
CTL# kubectl scale deployment web --replicas=4
N# iptables-save -t nat | grep "$SVC_CHAIN"
```

**Expected probabilities:** 0.25 → 0.333 → 0.50 → (implicit 1.0)
Verify each pod gets 25%:
- 0.25 → pod 1 gets 25%
- 0.333 of remaining 75% = 25% → pod 2
- 0.50 of remaining 50% = 25% → pod 3
- 100% of remaining 25% = 25% → pod 4

### Step 4 — Check packet counters on the service chain

```bash
N# iptables -t nat -L "$SVC_CHAIN" -n -v
```

Send some traffic, then re-check counters:

```bash
CTL# for i in $(seq 1 20); do curl -s http://10.96.45.123 > /dev/null; done
N# iptables -t nat -L "$SVC_CHAIN" -n -v
```

The `pkts` column tells you how many packets each KUBE-SEP was selected.

---

## 5. Phase 4: NodePort — External Traffic to Pod

### Step 1 — Convert the Service to NodePort

```bash
CTL# kubectl patch svc web -p '{"spec":{"type":"NodePort"}}'
CTL# kubectl get svc web
# NAME   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
# web    NodePort   10.96.45.123   <none>        80:30XXX/TCP   5m
```

Note the NodePort (e.g., `30080`).

### Step 2 — Find the KUBE-NODEPORTS rule

```bash
N# NODEPORT=$(kubectl get svc web -o jsonpath='{.spec.ports[0].nodePort}')
N# iptables-save -t nat | grep "$NODEPORT"
```

**You should see:**
```
-A KUBE-NODEPORTS -p tcp --dport 30080 -j KUBE-SVC-XXXXXX
```

The NodePort chain **reuses the same KUBE-SVC chain** as ClusterIP. Same load balancing, same pods.

### Step 3 — Find where KUBE-NODEPORTS is triggered

```bash
N# iptables-save -t nat | grep "KUBE-NODEPORTS"
# -A KUBE-SERVICES -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS
```

**`--dst-type LOCAL`** means: "destination IP is one of this node's IPs." This ensures any traffic landing on this node on a NodePort — regardless of which interface it arrived on — gets processed.

### Step 4 — Hit the NodePort from an external client

From your workstation or another machine:

```bash
curl http://<node-ip>:30080
```

### Step 5 — Find the MASQUERADE rule for cross-node traffic

```bash
N# iptables -t nat -L KUBE-POSTROUTING -n -v
```

**You'll see:**
```
MASQUERADE  all  --  anywhere  anywhere  /* kubernetes service traffic requiring SNAT */
```

**When this fires:** When a NodePort request arrives and the selected pod is on **a different node**, the packet must be forwarded. MASQUERADE rewrites the source to this node's IP so the reply comes back here (where conntrack can reverse the DNAT).

**When it doesn't fire:** If the pod is on the same node, MASQUERADE is skipped — no extra hop, client IP preserved.

### Step 6 — Observe cross-node MASQUERADE in conntrack

After hitting the NodePort from an external client:

```bash
N# conntrack -L | grep $NODEPORT
```

Two NAT layers will be visible:
1. DNAT: `<client-ip>:PORT → <node-ip>:30080` → `<pod-ip>:80`
2. SNAT (MASQUERADE): `<client-ip>` → `<this-node-ip>` (if cross-node)

---

## 6. Phase 5: Scale and Watch — Rules Change Live

### Step 1 — Open two terminals

**Terminal A (rule watcher):**
```bash
N# watch -n 2 'iptables-save -t nat | grep "^-A KUBE-SVC\|^-A KUBE-SEP" | wc -l'
```

**Terminal B (endpoint watcher):**
```bash
CTL# kubectl get endpoints web -w
```

### Step 2 — Scale up and observe

```bash
CTL# kubectl scale deployment web --replicas=6
```

**Watch Terminal A:** The KUBE-SVC + KUBE-SEP rule count increases as each new pod is added.
**Watch Terminal B:** New endpoint IPs appear in real time.

### Step 3 — Scale down and observe

```bash
CTL# kubectl scale deployment web --replicas=1
```

**Watch Terminal A:** Rule count drops as kube-proxy removes KUBE-SEP chains.
**Watch Terminal B:** Endpoints disappear.

### Step 4 — Correlate endpoints to iptables

```bash
CTL# kubectl get endpoints web -o jsonpath='{.subsets[0].addresses[*].ip}'
N# iptables-save -t nat | grep "KUBE-SEP" | grep "DNAT"
```

Every IP from kubectl endpoints should appear exactly once in a DNAT rule.

### Step 5 — Delete a pod manually and watch the chain disappear

```bash
CTL# POD=$(kubectl get pods -l app=web -o name | head -1)
CTL# kubectl delete $POD
```

Within ~5 seconds:
- kube-proxy removes the KUBE-SEP chain for that pod's IP
- Traffic stops going to the deleted pod

```bash
N# iptables-save -t nat | grep "DNAT --to-destination <old-pod-ip>"
```

**Expected:** No output — the rule is gone.

---

## 7. Phase 6: Conntrack in Kubernetes

### Step 1 — Generate sustained traffic to the Service

```bash
CTL# while true; do curl -s http://10.96.45.123 > /dev/null; sleep 0.5; done &
```

### Step 2 — Watch conntrack grow

```bash
N# watch -n 1 'conntrack -L 2>/dev/null | grep "10.96.45.123\|10.42" | wc -l'
```

### Step 3 — Read a full conntrack entry

```bash
N# conntrack -L | grep "10.96" | head -1
```

**Anatomy of a Kubernetes conntrack entry:**
```
tcp  6  86  TIME_WAIT
  src=10.42.0.1  dst=10.96.45.123  sport=54321  dport=80
  src=10.42.1.5  dst=10.42.0.1    sport=80      dport=54321
```

**Reading it:**
- Forward: pod/node sent to ClusterIP 10.96.45.123:80
- Reply: actual pod 10.42.1.5:80 responds back to the original source

The DNAT translation is embedded in conntrack — no KUBE-SVC lookup needed for established connections.

### Step 4 — The stale conntrack problem

**Simulate:** Delete a pod that has active connections:

```bash
CTL# kubectl delete pod <pod-name> --force
```

Check conntrack:

```bash
N# conntrack -L | grep "<deleted-pod-ip>"
```

**Entries may still exist!** These connections will hang until:
- The conntrack entry expires (TCP: varies; UDP: 30s)
- You manually clear them: `conntrack -D --dst <pod-ip>`

**Why this matters:** After scaling down, clients with long-lived connections may try to reuse a dead pod. This is why Kubernetes has `terminationGracePeriodSeconds` — to let connections drain before killing the pod.

---

## 8. Phase 7: NetworkPolicy — CNI Enforces, Not kube-proxy

NetworkPolicy is **not** implemented by kube-proxy. It's implemented by your CNI plugin. This phase shows where the enforcement happens.

### Step 1 — Identify your CNI

```bash
CTL# kubectl get pods -n kube-system
```

Look for: `calico`, `cilium`, `flannel`, `weave`, `canal`

### Step 2 — If using Calico — find cali-* chains

```bash
N# iptables-save -t filter | grep "^:cali" | head -10
```

**Calico adds chains like:**
```
:cali-FORWARD
:cali-INPUT
:cali-OUTPUT
:cali-fw-cali<veth-id>    ← from-workload (traffic leaving pod)
:cali-tw-cali<veth-id>    ← to-workload (traffic arriving at pod)
:cali-pi-<policy-id>      ← per-policy ingress rules
:cali-po-<policy-id>      ← per-policy egress rules
```

### Step 3 — Apply a NetworkPolicy and watch chains change

```bash
CTL# cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-ingress
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: web
  policyTypes:
  - Ingress
EOF
```

```bash
N# iptables-save -t filter | grep "cali-pi"
```

Calico has updated its `cali-pi-*` (policy-ingress) chain to drop traffic not explicitly allowed.

### Step 4 — Verify enforcement

From another pod, try to curl the web service:

```bash
CTL# kubectl run test --image=alpine --restart=Never -it --rm -- \
    wget -qO- http://web.default.svc.cluster.local --timeout=3
```

**Expected:** Timeout — NetworkPolicy blocked it at the iptables level on the node running the `web` pod.

### Step 5 — Allow traffic and see chain update

```bash
CTL# cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-test-to-web
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: web
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          run: test
    ports:
    - protocol: TCP
      port: 80
EOF
```

```bash
N# iptables-save -t filter | grep "cali-pi" -A 5
```

The allow rule is now visible in the chain.

### Step 6 — Clean up NetworkPolicies

```bash
CTL# kubectl delete networkpolicy deny-all-ingress allow-test-to-web
```

---

## 9. Phase 8: Debug "Service Not Reachable"

Work through this checklist when a Service doesn't respond.

### Step 1 — Does the Service have endpoints?

```bash
CTL# kubectl get endpoints <service-name>
```

**If EMPTY:**
```bash
CTL# kubectl get svc <service-name> -o yaml | grep -A5 selector
CTL# kubectl get pods --show-labels
```

The pod labels must match the Service's selector exactly.

### Step 2 — Is the iptables rule present?

```bash
N# SVC_IP=$(kubectl get svc <name> -o jsonpath='{.spec.clusterIP}')
N# iptables-save -t nat | grep "$SVC_IP"
```

**If missing:** kube-proxy hasn't synced. Check its status:

```bash
N# ps aux | grep kube-proxy
CTL# kubectl logs -n kube-system -l k8s-app=kube-proxy | tail -20
```

### Step 3 — Can you reach the pod directly?

```bash
CTL# POD_IP=$(kubectl get endpoints <name> -o jsonpath='{.subsets[0].addresses[0].ip}')
CTL# kubectl run test --image=alpine --restart=Never -it --rm -- \
    wget -qO- http://$POD_IP:<port> --timeout=3
```

**If direct pod access works but ClusterIP doesn't:**
→ iptables/kube-proxy issue. Rules exist but something is wrong in the chain.

**If direct pod access also fails:**
→ CNI issue or the app inside the pod is broken.

### Step 4 — Check KUBE-FORWARD allows forwarded traffic

```bash
N# iptables -L KUBE-FORWARD -n -v
```

This chain allows forwarded traffic with kube-proxy marks:

```
ACCEPT  all  ctstate RELATED,ESTABLISHED mark match 0x4000/0x4000
ACCEPT  all  mark match 0x4000/0x4000
```

If this chain is empty or missing, kube-proxy may not have run `--masq-all=false` correctly.

### Step 5 — Check for overlapping DROP rules

**Common mistake:** Adding a manual iptables DROP before kube-proxy's DNAT fires:

```bash
N# iptables -L FORWARD -n -v --line-numbers
N# iptables -L INPUT -n -v --line-numbers
```

Look for DROP rules that might be matching your traffic before kube-proxy's rules.

> **Remember:** kube-proxy operates in the **nat** table (PREROUTING). Your manual DROP rules in the **filter** table (FORWARD/INPUT) see the packet **after DNAT has already rewritten the destination**. A DROP on the original ClusterIP won't work — you must match on the pod IP.

### Step 6 — Is DNS resolving?

```bash
CTL# kubectl run test --image=busybox --restart=Never -it --rm -- \
    nslookup <service-name>.default.svc.cluster.local
```

DNS goes through CoreDNS → CoreDNS returns the ClusterIP → then iptables handles the routing. If DNS fails, check CoreDNS:

```bash
CTL# kubectl get pods -n kube-system -l k8s-app=kube-dns
CTL# kubectl logs -n kube-system -l k8s-app=kube-dns | tail -20
```

---

## 10. Phase 9: IPVS — When iptables Is Replaced

### Step 1 — Check your cluster's kube-proxy mode

```bash
CTL# kubectl get configmap kube-proxy -n kube-system -o yaml | grep -A3 mode
```

- `mode: ""` or `mode: "iptables"` → standard iptables mode
- `mode: "ipvs"` → IPVS mode
- k3s uses its own in-process proxy — no kube-proxy pod

### Step 2 — If IPVS mode, inspect with ipvsadm

```bash
N# ipvsadm -Ln | head -40
```

**Output shows virtual servers (Services) and real servers (pods):**
```
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port  Forward Weight ActiveConn InActConn
TCP  10.96.45.123:80 rr
  -> 10.42.1.5:80       Masq    1      0          0
  -> 10.42.2.3:80       Masq    1      0          0
```

Compare to iptables mode: instead of chained DNAT rules, IPVS uses a hash table — O(1) lookup vs O(n) chain traversal.

### Step 3 — IPVS still needs iptables for SNAT

Even in IPVS mode:

```bash
N# iptables -t nat -L KUBE-POSTROUTING -n -v
```

MASQUERADE rules are still present — IPVS handles the DNAT, but SNAT/MASQUERADE for outbound traffic still uses Netfilter.

### Step 4 — Rule count comparison

```bash
N# iptables-save | wc -l   # Should be much lower in IPVS mode
N# ipvsadm -Ln | grep "^TCP\|^UDP" | wc -l   # One per Service
```

In IPVS mode: iptables manages ~50 rules regardless of cluster size. IPVS manages service routing with its hash table.

---

## 11. Cleanup

```bash
# Remove test workloads
CTL# kubectl delete deployment web
CTL# kubectl delete svc web
CTL# kubectl delete pod test 2>/dev/null

# Clear any manual iptables rules you added on the node
N# iptables -L FORWARD -n -v --line-numbers  # review first
# Remove specific rules you added:
# N# iptables -D FORWARD <rule-number>

# Restart kube-proxy to restore clean state
CTL# kubectl rollout restart daemonset kube-proxy -n kube-system 2>/dev/null || true
```

---

## Summary: What Each Phase Proved

| Phase | Concept | Key Command |
|-------|---------|-------------|
| 1 | kube-proxy manages hundreds of rules | `iptables-save \| wc -l` |
| 2 | ClusterIP is a virtual IP — only in iptables | `iptables-save -t nat \| grep "$SVC_IP"` |
| 3 | Load balancing uses probability math | `iptables-save -t nat \| grep "statistic"` |
| 4 | NodePort reuses ClusterIP chains + MASQUERADE | `iptables-save -t nat \| grep "NODEPORTS"` |
| 5 | Rules update live as pods scale | `watch 'iptables-save \| grep KUBE-SEP \| wc -l'` |
| 6 | Conntrack stores DNAT mappings for return traffic | `conntrack -L \| grep "10.96"` |
| 7 | NetworkPolicy is enforced by CNI, not kube-proxy | `iptables-save -t filter \| grep "cali"` |
| 8 | "Service not reachable" is debugged in 6 steps | Endpoints → iptables → pod IP → DNS |
| 9 | IPVS replaces DNAT chains but keeps MASQUERADE | `ipvsadm -Ln` |

---

**Related docs:**
- [kube-networking-iptables.md](kube-networking-iptables.md) — the theory behind every rule you observed
- [Docker-Networking-iptables.md](Docker-Networking-iptables.md) — same patterns at container level
- [nat-lab.md](nat-lab.md) — DNAT and MASQUERADE fundamentals used throughout
